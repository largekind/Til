# NNを用いた強化学習

細かい部分は前述。
基本的には行動毎の価値をもとに動作を決める「価値関数ベース」と
行動そのものを決定して学習する「方策ベース」がある。

QDNは「価値関数ベース」で価値そのものを産出する価値関数をNNで大体させる形でニューラルネットワークを導入していく

## DQN

ある状態における行動を入力、その行動価値の出力部分(Q関数)をディープラーニングで計算させる手法

おもにQ network + Target Q Networkの２つのNNを用いて、生徒と教師みたいな形で学習を進めていく

### Q Network

現状態から最適な行動を行うためのQ関数を産出、行動を決定させるためのネットワーク

### Target Q Network

次の状態を受け取り、その状態の価値を算出するネットワーク。

次の価値を予め求めて、その値を教師ラベルとすることで、Q Networkを更新していく形式になる。

ただし、毎ステップで価値が変わると教師があやふやになるので、何ステップかで一気に学習する形式となっている（fixed target q-network)

### Experience reply

ランダムにシャッフルしてミニバッチを作成する手法。
時系列的に相関したデータをうけとって学習すると新しい環境に対応しづらいので、その対策
### fixed target q-network

毎ステップで教師がぶれぶれな答えを出すのを抑制するため、特定ステップごとで学習をTarget Q-networkを行わせる手法

### reward clipping

与える報酬を±1にクリッピングする手法。タスク間での報酬の価値が分からなくなるが、学習がしやすくなるメリットがある。

## 強化学習モデル

### ApeX

ゲームじゃないぞ

Networkを複数の演者(actor)と、その演者から出てきた多様な経験・ノウハウを取得できるLearnerに分け、
Learnerから定期的に今までに学んできた情報をもらいながら複数のActorが行動をしていく形のモデル

監督(Learner)が得てきた経験などがまとまったマニュアルをもとに実際に労働者(Actor)が動き、
Actorの意見や経験を監督が受け取りマニュアルに反映、そのマニュアルをもとに...といった形式が近い気がする

#### 優先度付き経験再生

学習がより早く進む「経験」をサンプリングする。つまり、間違えた問題や知らない問題といった
TD誤差が大きくなりそうな物については優先的にサンプリングしていく

ただし、毎回同じ大きい誤差をサンプリングされたら困るので、ある程度重み付けはする（**重大度サンプリング**)

#### Dueling Network

状態価値関数とQ値を分けて出してからQを求める形式にすることで、
選択する行動によって「価値」が変わる状態と変わらない状態を分ける

どこで行動が別れるかをきちんと考えるためのネットワーク

#### Double Deep Q-Network

通常のQ Networkが出す「最大価値を出すはずの行動」をTarget Q-Networkに渡し、
その行動における次状態の価値を算出することで、Q NetworkとTarget Q-Networkの差異が広がりすぎるのを防ぐ手法

#### multi-step bootstrap target

nステップ先の報酬もあらかじめ推測、算出してLossにする手法。
ある程度先も考慮した誤差になる分、誤差が大きくなるデメリットがある

### R2D2/R2D3

ApexにLSTM導入することで、時系列的な情報も学習できるようにしたモデル。
R2D3はR2D2にさらに教師データに対して実際のプロ（人間）の経験を導入したもの

### AlphaGo Zero(2017)

AlphaGoの2017最新モデル。囲碁のモデル

DQNでQ関数を求めるのも限界だったため、予め有力候補をMCTS(モンテカルロ木探索)から使って求めておき、
そこからQ関数を求める形にしたモデル

学習の流れとしては以下のとおりである
1. MCTSによる自己対局
2. DNNによる学習
3. DNNの対局による評価