---
title: "Global Attention Model"
date: 2023-04-06T00:00:00+09:00
lastmod: "2023-04-06"
---
# Global Attention Model

グローバルアテンションモデル

RNNといった系列モデルでは、最初の時刻に入力したデータが最後のほうまで伝わりづらい問題がある。

その対策として、以下の流れで計算してデコーダに注目箇所を渡す手法がある
1. デコーダの対象時刻の隠れ層出力とエンコーダの各時刻の出力から重みベクトルを算出
2. 重みベクトルとデコーダの対象時刻の重み付き和を計算、何を注目したかを表すコンテキストベクトルを作成
3. デコーダーの計算にコンテキストベクトルを渡す

## 重みベクトルの計算処理

以下の実装で重みベクトルを求められる

``` python
import numpy as np

def forward(self, hs, h):
  #hs : エンコーダにおける,すべての隠れ層の状態
  #h : デコーダにおける,ある時刻の隠れ状態 shape=(データ数N,隠れ層の重みH)

  N, T ,H = hs.shape #hsのshape = (データ数N, 時刻T, 隠れ層重みH)で構成

  # デコーダのある場所の隠れ状態hを3次元配列化
  # h = (N,H)の次元のため、reshapeメソッドで(N,1,H)に拡張、それをrepeatメソッドでaxis=1の軸でT分繰り返すことで、(N,T,H)次元のベクトルを作成する
  hr = h.reshape(N,1,H).repeat(T, axis = 1)
  
  #エンコーダの隠れ状態とデコーダの隠れ状態で内積
  t = hs * hr
  #データの重みの総和をとる (N,T,H)次元なので、axis=2で総和を得る
  s = np.sum(t,axis =2)

  #ソフトマックス関数を通して正規化
  a = softmax(s)

  return a

```

