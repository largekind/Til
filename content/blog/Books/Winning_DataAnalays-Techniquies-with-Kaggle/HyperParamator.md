---
title: "パラメータチューニング"
date: 2023-04-05T00:00:00+09:00
tags: [Books,Winning_DataAnalays-Techniquies-with-Kaggle]
---
# パラメータチューニング

モデルを学習するためのパラメータ（ハイパーパラメータ）を設定すること。

主に精度向上とかで使用される

ハイパラと略される

## ハイパーパラメータの探索手法

### 手動での探り方

#### グリッドサーチ

いくつかのパラメータ値を決め、それらの組み合わせで調整を行う。

ある程度、決め打ちが入るため計算速度は早いが、確実に最適なパラメータになる保障はない

たくさん指定すると流石に時間かかるので、その場合はランダムサーチ推奨

#### ランダムサーチ

ランダムでパラメータ値を決めて調整を行う手法

ランダムなので、最適なパラメータになる可能性が高い分、全探索に近いので計算時間がかかる

#### グリッドとランダムどっちがよい？

指定するハイパラの範囲がせまい場合はグリッドサーチ

逆にハイパラの範囲が広い場合はランダムサーチがよい

### 自動での方法

#### ベイズ探索

精度がよくなりそうなパラメータを効率よく探索する手法

ベイズ理論なので、確率分布を使用して検証し最適化するもの

ライブラリとしてはoptuna/hyperoptが使用される

※Optuna : 日本製

## パラメータチューニングで設定すること

### ベースラインのパラメータ

**過去のコンペや類似タスクを見る**

- 手動、自動での探索共通として、経験則的に良さそうなものを知っておき、ベースラインのパラメータやスコアを把握する必要がある
- デフォルト値が適切かは見ておく
  - xgboostの学習率は0.3とデフォルトで極端に大きいなど、罠がある
  - 過去のコンペなども確認し、使えそうなパラメータ値を見ておこう

### 手動で気を付けること

train/validでデータ分布がどう変わるかは考えておく

普通に自動の方法を使うのを推奨

### 自動で気を付けること

特になし。チューニングし過ぎによる過学習に注意

## パラメータチューニングのコツ

- 重要なパラメータから調整する
  - 例 : 木構造 max_depth -> min_child_weightなど
- パラメータを増やすことによる影響を理解する
  - max_depth 増やす -> 複雑化/ drop_rate増やす -> 単純化など
- パラメータ探索時の上下限に集中する。
  - 例 : 0 - 0.5で探索して、0.5ばかり最適パラメータが出る場合、0.5以降のより大きい値が最適値になっている可能性がある
- シード値はきちんと決めておく
  - 設定しないと再現できなくなる
  - シードによる精度の誤差に対する対策が可能となる

## ベイズ最適化でのパラメータ探索

Kaggle分析の本ではhyperoptが使用されている…が分析コンペでは使われておらず、Oputunaを使用

Optuna/hyperopt問わず、使い方を学んでおく

### 使用法

1. 最小化したい評価指標を決定
2. 探索するパラメータ範囲を決定
  - 決定木は一様分布
  - 正則化の強さは0.1/0.01/0.001...といった感じでn倍間隔がおすすめ
3. 探索回数を決定

#### hyperoptと比較したOptunaの利点

- Define by RunでのAPI設計となっている
- 学習曲線を用いた試行の枝刈り(pruning）がある

## GBDTのパラメータチューニング

GBDT : 勾配ブースティング木

今回は例としてXGBoostを使用

- 基本的に言えること
  - 学習率は最初は大きめ、精度を求める段階で徐々に小さくするのがよい
  - num_round(epoch/学習回数)は大きく、early_stoppingで学習を止める形式にする
  - Analytics Vidhyaの記事にも詳細されているので、検索して調べてみる

## Neaural Networkモデルのパラメータチューニング

- ネットワーク構成や層などが調整できる
- Optimizerの選択
  - Adamでよい気がする…
  - 最近はAdamよりもよいLionと呼ばれる最適化手法も出てきた模様
    - [github](https://github.com/lucidrains/lion-pytorch)

## 線形モデルによるチューニング

正則化になる

Lasso/Ridge/Elastic Net正則化が入る




