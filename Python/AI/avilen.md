
# CNN

自研究で使用していたやつ。Convolution Neural Network
下記の流れで特徴マップを学習していく手法

> 入力 -> 畳み込み演算 -> プーリング -> 畳み込み演算 -> ... -> 最終的な出力

## 畳み込み演算

フィルタで行列計算をするアレ 詳細はGoogle
主に情報の抽出に使用

### プーリング

データの圧縮で使用するもの
情報の統合により、大域的な情報を抽出する

- 最大値プーリング(max pooling) : フィルタでのmax値を出す
- 平均値プーリング(average pooling) :フィルタでのmean値を出す

### im2col

画像を行列計算するために列に分割する処理。
行列の内積を算出して一気に畳み込み演算を行う
フィルタを適用する画像領域を一行化する処理することで、
１行化した画像領域と１列化したフィルタで行列積算が可能となり、計算が高速化する

## 正規化（画像）

ネットワークが深くなると、非線形変換の繰り返しによりデータ分布が変わることがある（**内部の共変量シフト**）
### Batch Normalization

全てのバッチごとに正規化(平均0、分散1化)する処理
各ノードの値をミニバッチ単位で正規化して、スケールをそろえる（バッチ正規化）
**正則化としても機能する** (L2正則化やDrropoutの必要性が小さくなる)

### Layer Normalization

レイヤーごとに正規化を行う処理
オンライン学習やRNNで使用する

### Instance Normalization

各チャネルで独立で画像の縦横方向のみで平均・分散をとる
画像分野ではバッチ正規化の代わりで注目されている

### Group Normalization

チャネルをG個でグルーピングしてLayer Normalizatoin/Instance Normalizationの中間的な正規化を行う
セグメンテーションなどで利用

# RNN

時系列データを取り扱うDL系列。RNNとかLSTMとかGRUとか

## Recurrent Neaural Network (RNN)

Reccurent : 回帰
回帰的に前の入力情報を次のネットワークに伝搬させることで、時系列データの学習を可能にしたDNN

## BPTT (Backpropagation Through Time)

時間方向の逆伝搬を行う処理
前時間の入力を別の追加したネットワークに入れこむ形にし、伝搬可能にすることで実現している

## RNNの逆伝搬

連鎖律を用いて計算していく形式でしていく。計算が難しいので検索して理解しておく

## 勾配消失

よくある層が深くなると学習ができなくなってしまうやつ。
原因は「活性化関数」を通ることで出力されるデータが段々少なくなるため
RNNでも最後の出力で活性化関数を通る関係で勾配消失が発生する

## 勾配爆発

前の情報が強すぎて逆に勾配が爆発してしまう現象
原因対策として、一定以上の勾配は閾値を取って頭打ちにする**勾配クリッピング**を用いる

## 勾配クリッピング

L2ノルムを用いて、勾配が一定以上にならないように頭打ちを行う処理

## RNNの伝搬

### 順伝搬

普通に計算。ただの計算グラフなのでそこまで考えない
**入力する地点で重みとバイアスが入り込むことを忘れないようにする**

### 逆伝搬

**登場するパラメータと出力値で連鎖律を行う**
順伝搬で使っていた+/*の計算やtanhといった活性化関数に惑わされないようにする

# AE

## Auto Encoder

生成モデルの一種。EncoderとDeconderに分かれ、適当な入力に応じて何かしらを出力するモデル
Encoder -> 潜在変数z -> Decoderの形でzに次元圧縮して何かしらの特徴に圧縮して、Decoderでその圧縮された特徴を用いてデータを出力する

入ったものを圧縮してそのまま出すしかしてないので、**中間的な画像が出せないデメリットがある**

## Variational Autoencoder

通称VAE
EncoderとDecoderの間にあるzを確率分布として平均、分散を求め、そこからサンプリングでzを取る形式になったもの

確率変数をいじれば中間的な画像を連続的に出せる

### Reparametrization Trick

VAEの学習で逆伝搬をするために、**ガウシアンノイズを用いてサンプリングを近似的に行うことで、逆伝搬可能にした方法**
直訳すると「再パラメータ化トリック」というだけあって、サンプリングの処理をガウシアンノイズのパラメータで行うトリックを用いている

### posterior collpage

**VAEの問題** 表現力が高いDecoder(例:PixelCNN)などを用いると潜在変数が無視された生成が出てしまう
対策として潜在変数zを離散的なベクトルに変えたVQ-VAEなどがある

## VQ-VAE

潜在変数に離散ベクトルを用いて学習させることで、上記の問題を解決させたVAE

## GAN

いつもの。Generative Adversal Network(敵生成ネットワーク)
GeneratorとDiscriminatorを戦闘させて学習していくニセ札と警察の関係的な発想で生まれたネットワーク

# 強化学習

## 教師あり学習

正解がわかっているデータをもとに、入力データと対応する正解の出力の関係を学習する
画像分類などが該当

## 教師なし学習

与えられたデータが持つ構造そのものを学習。正解そのものが無い
SVMやクラスタリングが該当
## 強化学習

**報酬**をもとに、最適な意思決定を作成するための学習方法
五目並べとか遺伝子的アルゴリズムとかが該当

## マルコフ決定過程

「状態」 + 「行動」+ 「報酬」で最適化を行う処理
確率的に変化させる決定に報酬や行動によるウェイトを追加したもの

### マルコフ過程

「状態」で一意に決まる確率過程のこと
数学者であるアンドレイ・マルコフにちなんで命名された
これに「行動」と「報酬」が入るとマルコフ決定過程になる
### マルコフ性

今の状態に応じて次の状態が決定するという性質。
*今*が次を決めるやつ。過去の自分に影響されたり、未来からドラ〇もんが来て状態が勝手に変化するとかは無いよっていうやつ

## 方策

どう行動を決定するかを考えるための決まりや戦略の基準となる部分。

### 割引率

**未来に対する報酬に対する影響に対して重みづけを行うための報酬のウェイト値**
直近だけでなく、未来の報酬もある程度考慮して考えるための影響度の度合い

### 累積報酬和

価値を表す総和の式。
R = 直近の報酬 + 次の報酬 * 割引率 + さらに次の報酬 * 割引率^2 + ... = 報酬*時間分の割引率の総和

上を価値関数として見て、最大化するのが目的となる


### 価値関数

強化学習における目的関数のこと
実質的に累積報酬和をmaxへ持っていくための行動と状態を表す式が価値関数になる

### 最適ベルマン方程式

*「今の価値」* + **今後の価値の期待値**で決まる価値関数
「価値を最大化する行動」 = 「価値を最大かする最適な方策」を結びつけることが出来る

## 強化学習におけるアプローチ

### 価値関数ベース

価値を最大化するような行動をする（最適な行動をする）のを軸に評価するアプローチ
**価値そのものをモデル化**して、行動自体は別のアルゴリズムに代用させる形式

### 方策ベース

価値を最大するような行動ルール(方策)を見つけようとするのを軸に評価するアプローチ
**方策（行動の選択方法）をモデル化**する方式

## 価値関数ベースのアプローチ

### 行動価値関数(Q)

一定の条件時での価値関数の値を求める関数。
ある状態である状態を取ったときの価値に限定した方法

### 価値関数法

価値関数をもとに行動を選択、試行を繰り返して価値関数を更新する手法
**環境に対する知識(環境ダイナミクス)が完璧にある時に使用できる**

#### 環境ダイナミクス

状態転移確率と報酬が既知である状態
迷路とか完璧に設計されているような、既に分かっている問題などが該当

### 環境のサンプリング

**実際に行動してみて、その得た情報で価値関数を更新する方式**
とりあえずやってみよう理論

#### モンテカルテ法

最後までプレイして、その報酬から価値を推定する方式

#### TD法(Temporal Difference Learning)

その時までに得た報酬で価値関数を更新する手法
(即時報酬で更新していく)


### 局所解への対策

強化学習は初期の方策（行動）に依存しやすい傾向があるため、
たまには今までの学習を無視して行動させようという対策がある

#### ε-greedy方策

確率εでランダム行動させる方策

#### ソフトマックス方策

ある程度の今までの学習を用いて、ボルツマン分布に従い行動を設定させる
（完全ランダムではなく、ある程度今までの学習も信用してランダムな行動をする）

## TD法の分類
### Q学習

**方策OFF型**
次の状態がどれくらいの価値を持つかを*現在推定されている値のMax*を用いてQ値を更新する手法
「明日は自分はベストな行動取るだろう」と信じて学習していく手法
最適なベスト値(Max値)を取っているので学習が安定する。

### SARASA

**方策OFF型**
今の方策に従って値を更新していく手法
「未来より、今の自分を信じる」として学習していく手法
現在の方策で得た値を取る(ランダム性がある)ため学習が若干不安定だが、局所解に陥る危険が少ない


## 方策勾配法(方策ベース)

土の手をどれくらいの確率で選択すべきかの**方策**自体を直接予測
**未来の盤面を列挙しない** + 相性の良い手法と組み合わせて、性能が良い方策を実現可能！

### 方策勾配定理

方策パラメータΘに対する偏微分で得られる方策勾配が満たす性質のこと
**行動価値関数のQ値が分かればベストな方策がわかるよ定理**

価値関数をΘで偏微分するメチャクチャ面倒な式なので覚えたほうが早い
*デメリットとしてQ値を求めるのが難しいので、Q値の平均値などを工夫して必要がある*

#### ベースライン関数

現在の報酬からベース分引いて、分散を抑えるための関数

#### アドバンテージ関数

行動価値関数から推定される価値関数を引くことで、ある程度の分散を抑える関数


## 方策ベースのアプローチ

### REINFORCEアルゴリズム

REINFORCE : 強化
実際に得られた報酬の平均を使って近似したものをQ値とする方法
「死ぬたびに学習→強くなる」方式

### Actor Critic

Actor Critic : 演者 - 評論家
状態価値をNeaural Networkを用いて推測させ、方策を決定させる方式
「演者が上手く演技しているか、評論家が評価して強くしていく」方式

### A3C

Asynchoronous advantage Actor Crinic の略
Actor Crinicに加えて、パラメータの更新を非同期で行う*Asynchrouns*と推定にAdvantageを用いる形式
(Asynchoorus + Advantage + Actor : A3)
「演者がどれだけ有利に事を運んで演技しているかを、評論家が一度にいっぱい見ながら評価して強くしていく」方式

#### A2C

A3Cの同期分散処理版モデル
Asynchorousが消えているのでA2C
**同期するのでGPUで使いやすいメリットがある**


#### ACER

A3Cベースに勾配オフ型にし、Experience Replay(経験再生)を利用したモデル
過去の履歴も学習できるようになり、更に収束しづらい問題は*Retrace*と呼ばれる手法で解決している

#### UNREAL

Unsupervied reinforcement and auxiliary learning : 教師なしでの補助学習
A3Cベースに補助タスクも同時学習させたモデル
「色んな学習を同時に強化学習させよう」モデル

# DL実用

## 事前学習(pre-training)

よくあるやつ。予め大量のデータセットでモデルを学習させておくこと

## 転移学習

学習済みのモデルに層を追加して、**追加した部分の箇所だけ学習させる**方式

## fine-tuning

よくあるやつその2。学習済みモデルの重みを初期値として**全体のモデルを再学習させる**方式
転移学習が一部に対し、fine-tuningは全体を学習させる

## ワンショット学習

データに対し1ラベルを付与した画像などで学習させる方式
例としては顔の学習など

## ゼロショット学習

ラベルを付与せず、特徴ベクトルなどに変換して学習を行わせる方式
Decoderとかが近そう

## 半教師あり学習

一部データだけラベル付与して最初は学習、
予測した結果を用いてラベルなしデータにラベル付けして更に学習を行う形式

## 自己教師あり学習

同じラベルの画像をデータ拡張してもラベルは同じなのを利用して、
データ拡張後の画像の予測が元画像の答えと近づくように学習していくような形式
ある意味Tripnet Networkに近い気がする（後述）

## マルチタスク学習

昔自研究で検討しようとしたやつ。複数のタスクに対しての予測を行い学習をする形式

## 距離学習

2つ以上のデータ間の類似性を用いて学習すること
何を類似性にするかは色々種類があるので割愛。（例:ユーグリット距離とかハミング距離とか)

### Siamese Network

SIamese : シャムと読む シャム双生児が由来の模様（頭２つ、体１つの奇形児）
同じペアのデータセットを作って、2つの入力を同じネットワークに入れ、
出力が同じになるように学習させていくネットワーク

### Triplet Network

Triplet : 三つ子
同じペア、違うペアで３人組を作って、
同じペアに出力値を近づけ、違うペアには出力が遠さがるような学習をしていくネットワーク

#### Easy Negative

3つのペアのサンプリングを行うケースの1つ
元画像に大して別ラベルの画像が元々遠い位置にサンプリングされてる場合、そもそも別カテゴリと見られて学習ができないもの

#### Semi-Hard Negative

3つのペアのサンプリングを行うケースの1つ
元画像に対し、同じラベルの画像と別ラベルの画像がいい塩梅でサンプリングされており、学習がしやすい状況のもの

#### Hard Negative

3つのペアのサンプリングを行うケースの1つ
元画像に対し、別ラベルの画像が元画像に近い場所にサンプリングされてしまい、
別ラベル画像が元画像と同じラベルの画像と誤認されてしまう状況のもの

## メタ学習(Meta-Learning)

どのように学習すればよりよくNeural Networkを学習できるかを考えること
メタ思考的な学習 モデルのハイパーパラメータどうするかetc...

### few-shot learning

少ない画像データで学習する手法のこと。
その代表的な手法としてMAMLが存在している。
#### MAML(Model-Agnostic Meta-Learning)

何度か小さいデータセットで試しに学習を行い、
その平均を初期値として取ることで学習の最適化を行わせる手法

## Explainable AI

説明可能なAIのこと
例としてHeatmapなどを用いて何を着目して判断したかを見る

### CAM

Global Average pooling(GAP)で最後の出力を出している層に着目、
そこの重みを各チャネルの画像として抽出して元画像と合成することでヒートマップを作る手法
GAPが最終層となっているResNetなどでしか使えない欠点がある
#### Grad-CAM

畳み込みの最終層の特徴に着目してヒートマップを作る手法
GAPがないネットワークでも使える利点があるため、現在の主流となっている

#### Guided-Grad CAM

GradCAMに加え絵、クラスに対しての勾配を求めて画像に入れこむ(Guided back propagation)することで、どこに着目しているのかを画像で見れる手法

### LIME

局所モデルを用意して、局所モデルを局所敵なデータで学習させ特徴量を見ることで、
そのモデルが何を見ているかを間接的にわかるようにしようという手法

### SHAP

シャープレイ値（各特徴量を組み合わせた時にどれだけ貢献値があるか）を近似的に計算して、
何が影響を与えているかを見れるようにした手法

## Graph Neaural Network

入力をグラフとして扱うNN。これにより構造そのものを表現し、学習可能となる。
(例 : 化合物、文章、ソーシャルネットワーク)
各グラフ間の関係をグラフ行列としてあらわして入力することで学習させている

### グラフ畳み込み

グラフに対する畳み込み計算を行うこと。
これにより、隣接するノードの関係など、様々な情報を抽出できる。
名前の通り、グラフ版Convolutionみたいなもの
### Relational Network

グラフ畳み込み計算を用いたニューラルネットワークのこと
関係毎に畳み込みを行い特徴を抽出する。
(活用例 : 企業ごとでの特徴や格付けの予測など)

