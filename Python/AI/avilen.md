
# CNN

自研究で使用していたやつ。Convolution Neural Network  
下記の流れで特徴マップを学習していく手法  

> 入力 -> 畳み込み演算 -> プーリング -> 畳み込み演算 -> ... -> 最終的な出力

## 畳み込み演算

フィルタで行列計算をするアレ 詳細はGoogle  
主に情報の抽出に使用

### プーリング

データの圧縮で使用するもの  
情報の統合により、大域的な情報を抽出する

- 最大値プーリング(max pooling) : フィルタでのmax値を出す  
- 平均値プーリング(average pooling) :フィルタでのmean値を出す

### im2col

画像を行列計算するために列に分割する処理。  
行列の内積を算出して一気に畳み込み演算を行う  
フィルタを適用する画像領域を一行化する処理することで、  
１行化した画像領域と１列化したフィルタで行列積算が可能となり、計算が高速化する

## 正規化（画像）

ネットワークが深くなると、非線形変換の繰り返しによりデータ分布が変わることがある（**内部の共変量シフト**）
### Batch Normalization

全てのバッチごとに正規化(平均0、分散1化)する処理  
各ノードの値をミニバッチ単位で正規化して、スケールをそろえる（バッチ正規化）  
**正則化としても機能する** (L2正則化やDrropoutの必要性が小さくなる)

### Layer Normalization

レイヤーごとに正規化を行う処理  
オンライン学習やRNNで使用する

### Instance Normalization

各チャネルで独立で画像の縦横方向のみで平均・分散をとる  
画像分野ではバッチ正規化の代わりで注目されている

### Group Normalization

チャネルをG個でグルーピングしてLayer Normalizatoin/Instance Normalizationの中間的な正規化を行う  
セグメンテーションなどで利用

# RNN

時系列データを取り扱うDL系列。RNNとかLSTMとかGRUとか

## Recurrent Neaural Network (RNN)

Reccurent : 回帰  
回帰的に前の入力情報を次のネットワークに伝搬させることで、時系列データの学習を可能にしたDNN

## BPTT (Backpropagation Through Time)

時間方向の逆伝搬を行う処理  
前時間の入力を別の追加したネットワークに入れこむ形にし、伝搬可能にすることで実現している

## RNNの逆伝搬

連鎖律を用いて計算していく形式でしていく。計算が難しいので検索して理解しておく

## 勾配消失

よくある層が深くなると学習ができなくなってしまうやつ。  
原因は「活性化関数」を通ることで出力されるデータが段々少なくなるため
RNNでも最後の出力で活性化関数を通る関係で勾配消失が発生する

## 勾配爆発

前の情報が強すぎて逆に勾配が爆発してしまう現象  
原因対策として、一定以上の勾配は閾値を取って頭打ちにする**勾配クリッピング**を用いる

## 勾配クリッピング

L2ノルムを用いて、勾配が一定以上にならないように頭打ちを行う処理

## RNNの伝搬

### 順伝搬

普通に計算。ただの計算グラフなのでそこまで考えない  
**入力する地点で重みとバイアスが入り込むことを忘れないようにする**

### 逆伝搬

**登場するパラメータと出力値で連鎖律を行う**  
順伝搬で使っていた+/*の計算やtanhといった活性化関数に惑わされないようにする

# AE

## Auto Encoder

生成モデルの一種。EncoderとDeconderに分かれ、適当な入力に応じて何かしらを出力するモデル  
Encoder -> 潜在変数z -> Decoderの形でzに次元圧縮して何かしらの特徴に圧縮して、Decoderでその圧縮された特徴を用いてデータを出力する  

入ったものを圧縮してそのまま出すしかしてないので、**中間的な画像が出せないデメリットがある**

## Variational Autoencoder

通称VAE  
EncoderとDecoderの間にあるzを確率分布として平均、分散を求め、そこからサンプリングでzを取る形式になったもの  

確率変数をいじれば中間的な画像を連続的に出せる

### Reparametrization Trick

VAEの学習で逆伝搬をするために、**ガウシアンノイズを用いてサンプリングを近似的に行うことで、逆伝搬可能にした方法**  
直訳すると「再パラメータ化トリック」というだけあって、サンプリングの処理をガウシアンノイズのパラメータで行うトリックを用いている

### posterior collpage

**VAEの問題** 表現力が高いDecoder(例:PixelCNN)などを用いると潜在変数が無視された生成が出てしまう  
対策として潜在変数zを離散的なベクトルに変えたVQ-VAEなどがある

## VQ-VAE

潜在変数に離散ベクトルを用いて学習させることで、上記の問題を解決させたVAE

## GAN

いつもの。Generative Adversal Network(敵生成ネットワーク)  
GeneratorとDiscriminatorを戦闘させて学習していくニセ札と警察の関係的な発想で生まれたネットワーク

# 強化学習

## 教師あり学習

正解がわかっているデータをもとに、入力データと対応する正解の出力の関係を学習する  
画像分類などが該当

## 教師なし学習

与えられたデータが持つ構造そのものを学習。正解そのものが無い  
SVMやクラスタリングが該当
## 強化学習

**報酬**をもとに、最適な意思決定を作成するための学習方法  
五目並べとか遺伝子的アルゴリズムとかが該当

## マルコフ決定過程

「状態」 + 「行動」+ 「報酬」で最適化を行う処理  
確率的に変化させる決定に報酬や行動によるウェイトを追加したもの

### マルコフ過程

「状態」で一意に決まる確率過程のこと  
数学者であるアンドレイ・マルコフにちなんで命名された  
これに「行動」と「報酬」が入るとマルコフ決定過程になる
### マルコフ性

今の状態に応じて次の状態が決定するという性質。  
*今*が次を決めるやつ。過去の自分に影響されたり、未来からドラ〇もんが来て状態が勝手に変化するとかは無いよっていうやつ

## 方策

どう行動を決定するかを考えるための決まりや戦略の基準となる部分。

### 割引率

**未来に対する報酬に対する影響に対して重みづけを行うための報酬のウェイト値**  
直近だけでなく、未来の報酬もある程度考慮して考えるための影響度の度合い

### 累積報酬和

価値を表す総和の式。  
R = 直近の報酬 + 次の報酬 * 割引率 + さらに次の報酬 * 割引率^2 + ... = 報酬*時間分の割引率の総和

上を価値関数として見て、最大化するのが目的となる


### 価値関数

強化学習における目的関数のこと  
実質的に累積報酬和をmaxへ持っていくための行動と状態を表す式が価値関数になる

### 最適ベルマン方程式

*「今の価値」* + **今後の価値の期待値**で決まる価値関数  
「価値を最大化する行動」 = 「価値を最大かする最適な方策」を結びつけることが出来る

## 強化学習におけるアプローチ

### 価値関数ベース

価値を最大化するような行動をする（最適な行動をする）のを軸に評価するアプローチ  
**価値そのものをモデル化**して、行動自体は別のアルゴリズムに代用させる形式

### 方策ベース

価値を最大するような行動ルール(方策)を見つけようとするのを軸に評価するアプローチ  
**方策（行動の選択方法）をモデル化**する方式

## 価値関数ベースのアプローチ

### 行動価値関数(Q)

一定の条件時での価値関数の値を求める関数。  
ある状態である状態を取ったときの価値に限定した方法

### 価値関数法

価値関数をもとに行動を選択、試行を繰り返して価値関数を更新する手法  
**環境に対する知識(環境ダイナミクス)が完璧にある時に使用できる**

#### 環境ダイナミクス

状態転移確率と報酬が既知である状態  
迷路とか完璧に設計されているような、既に分かっている問題などが該当

### 環境のサンプリング

**実際に行動してみて、その得た情報で価値関数を更新する方式**  
とりあえずやってみよう理論

#### モンテカルテ法

最後までプレイして、その報酬から価値を推定する方式  

#### TD法(Temporal Difference Learning)

その時までに得た報酬で価値関数を更新する手法  
(即時報酬で更新していく)


### 局所解への対策

強化学習は初期の方策（行動）に依存しやすい傾向があるため、  
たまには今までの学習を無視して行動させようという対策がある

#### ε-greedy方策

確率εでランダム行動させる方策

#### ソフトマックス方策

ある程度の今までの学習を用いて、ボルツマン分布に従い行動を設定させる  
（完全ランダムではなく、ある程度今までの学習も信用してランダムな行動をする）

## TD法の分類
### Q学習

**方策OFF型**  
次の状態がどれくらいの価値を持つかを*現在推定されている値のMax*を用いてQ値を更新する手法  
「明日は自分はベストな行動取るだろう」と信じて学習していく手法  
最適なベスト値(Max値)を取っているので学習が安定する。

### SARASA

**方策OFF型**  
今の方策に従って値を更新していく手法  
「未来より、今の自分を信じる」として学習していく手法  
現在の方策で得た値を取る(ランダム性がある)ため学習が若干不安定だが、局所解に陥る危険が少ない


## 方策勾配法(方策ベース)

土の手をどれくらいの確率で選択すべきかの**方策**自体を直接予測  
**未来の盤面を列挙しない** + 相性の良い手法と組み合わせて、性能が良い方策を実現可能！

### 方策勾配定理

方策パラメータΘに対する偏微分で得られる方策勾配が満たす性質のこと  
**行動価値関数のQ値が分かればベストな方策がわかるよ定理**  

価値関数をΘで偏微分するメチャクチャ面倒な式なので覚えたほうが早い  
*デメリットとしてQ値を求めるのが難しいので、Q値の平均値などを工夫して必要がある*

#### ベースライン関数

現在の報酬からベース分引いて、分散を抑えるための関数

#### アドバンテージ関数

行動価値関数から推定される価値関数を引くことで、ある程度の分散を抑える関数


## 方策ベースのアプローチ

### REINFORCEアルゴリズム

REINFORCE : 強化  
実際に得られた報酬の平均を使って近似したものをQ値とする方法  
「死ぬたびに学習→強くなる」方式

### Actor Critic

Actor Critic : 演者 - 評論家  
状態価値をNeaural Networkを用いて推測させ、方策を決定させる方式  
「演者が上手く演技しているか、評論家が評価して強くしていく」方式

### A3C

Asynchoronous advantage Actor Crinic の略  
Actor Crinicに加えて、パラメータの更新を非同期で行う*Asynchrouns*と推定にAdvantageを用いる形式  
(Asynchoorus + Advantage + Actor : A3)  
「演者がどれだけ有利に事を運んで演技しているかを、評論家が一度にいっぱい見ながら評価して強くしていく」方式

#### A2C

A3Cの同期分散処理版モデル  
Asynchorousが消えているのでA2C  
**同期するのでGPUで使いやすいメリットがある**


#### ACER

A3Cベースに勾配オフ型にし、Experience Replay(経験再生)を利用したモデル  
過去の履歴も学習できるようになり、更に収束しづらい問題は*Retrace*と呼ばれる手法で解決している

#### UNREAL

Unsupervied reinforcement and auxiliary learning : 教師なしでの補助学習  
A3Cベースに補助タスクも同時学習させたモデル  
「色んな学習を同時に強化学習させよう」モデル

# DL実用

## 事前学習(pre-training)

よくあるやつ。予め大量のデータセットでモデルを学習させておくこと

## 転移学習

学習済みのモデルに層を追加して、**追加した部分の箇所だけ学習させる**方式

## fine-tuning

よくあるやつその2。学習済みモデルの重みを初期値として**全体のモデルを再学習させる**方式  
転移学習が一部に対し、fine-tuningは全体を学習させる

## ワンショット学習

データに対し1ラベルを付与した画像などで学習させる方式  
例としては顔の学習など

## ゼロショット学習

ラベルを付与せず、特徴ベクトルなどに変換して学習を行わせる方式  
Decoderとかが近そう

## 半教師あり学習

一部データだけラベル付与して最初は学習、  
予測した結果を用いてラベルなしデータにラベル付けして更に学習を行う形式

## 自己教師あり学習

同じラベルの画像をデータ拡張してもラベルは同じなのを利用して、  
データ拡張後の画像の予測が元画像の答えと近づくように学習していくような形式  
ある意味Tripnet Networkに近い気がする（後述）

## マルチタスク学習

昔自研究で検討しようとしたやつ。複数のタスクに対しての予測を行い学習をする形式

## 距離学習

2つ以上のデータ間の類似性を用いて学習すること  
何を類似性にするかは色々種類があるので割愛。（例:ユーグリット距離とかハミング距離とか)

### Siamese Network

SIamese : シャムと読む シャム双生児が由来の模様（頭２つ、体１つの奇形児）  
同じペアのデータセットを作って、2つの入力を同じネットワークに入れ、  
出力が同じになるように学習させていくネットワーク

### Triplet Network

Triplet : 三つ子  
同じペア、違うペアで３人組を作って、  
同じペアに出力値を近づけ、違うペアには出力が遠さがるような学習をしていくネットワーク

#### Easy Negative

3つのペアのサンプリングを行うケースの1つ  
元画像に大して別ラベルの画像が元々遠い位置にサンプリングされてる場合、そもそも別カテゴリと見られて学習ができないもの

#### Semi-Hard Negative

3つのペアのサンプリングを行うケースの1つ  
元画像に対し、同じラベルの画像と別ラベルの画像がいい塩梅でサンプリングされており、学習がしやすい状況のもの

#### Hard Negative

3つのペアのサンプリングを行うケースの1つ  
元画像に対し、別ラベルの画像が元画像に近い場所にサンプリングされてしまい、  
別ラベル画像が元画像と同じラベルの画像と誤認されてしまう状況のもの

## メタ学習(Meta-Learning)

どのように学習すればよりよくNeural Networkを学習できるかを考えること  
メタ思考的な学習 モデルのハイパーパラメータどうするかetc...

### few-shot learning

少ない画像データで学習する手法のこと。  
その代表的な手法としてMAMLが存在している。
#### MAML(Model-Agnostic Meta-Learning)

何度か小さいデータセットで試しに学習を行い、  
その平均を初期値として取ることで学習の最適化を行わせる手法

## Explainable AI

説明可能なAIのこと  
例としてHeatmapなどを用いて何を着目して判断したかを見る

### CAM

Global Average pooling(GAP)で最後の出力を出している層に着目、  
そこの重みを各チャネルの画像として抽出して元画像と合成することでヒートマップを作る手法  
GAPが最終層となっているResNetなどでしか使えない欠点がある
#### Grad-CAM

畳み込みの最終層の特徴に着目してヒートマップを作る手法  
GAPがないネットワークでも使える利点があるため、現在の主流となっている

#### Guided-Grad CAM

GradCAMに加え絵、クラスに対しての勾配を求めて画像に入れこむ(Guided back propagation)することで、どこに着目しているのかを画像で見れる手法

### LIME

局所モデルを用意して、局所モデルを局所敵なデータで学習させ特徴量を見ることで、  
そのモデルが何を見ているかを間接的にわかるようにしようという手法

### SHAP

シャープレイ値（各特徴量を組み合わせた時にどれだけ貢献値があるか）を近似的に計算して、  
何が影響を与えているかを見れるようにした手法

## Graph Neaural Network

入力をグラフとして扱うNN。これにより構造そのものを表現し、学習可能となる。  
(例 : 化合物、文章、ソーシャルネットワーク)  
各グラフ間の関係をグラフ行列としてあらわして入力することで学習させている

### グラフ畳み込み

グラフに対する畳み込み計算を行うこと。
これにより、隣接するノードの関係など、様々な情報を抽出できる。
名前の通り、グラフ版Convolutionみたいなもの
### Relational Network

グラフ畳み込み計算を用いたニューラルネットワークのこと  
関係毎に畳み込みを行い特徴を抽出する。  
(活用例 : 企業ごとでの特徴や格付けの予測など)

# 画像認識

## 画像分類

いつもの。画像からクラス分類するだけ

## 物体検出

YOLOとかのやつ。画像から物体が「どこに」、「それが何か」を示すやつ

### IoU(Intersection over Union)

Intersection : 共通部分  
over Union : 和集合  
物体検出に使われている評価指標。「どこに」を示す評価指標。  
予測した領域と正解領域の共通部分が一致している度合いをパーセンテージで表す。

## セグメンテーション

ピクセル単位の物体検出

## 画像分類用データセット

### MINST

数字0-9のデータ

### CIFER-10/CIFER-100

乗り物や動物のデータセット。後ろの10/100はクラス数  
ピクセルが32*32と少し小さめ

### Fashon-MINST

服とかの画像

### Food101

食べ物101個

### Image-Net

ジャンル問わないバカみたいに大きい画像データセット  
2万クラスあるので、事前学習させたモデル使ったりとか

## 物体検出/セグメンテーション用データセット

### Pascal VOC dataset

基本的なデータセット。カテゴリ20で1万程度ある

### COCO-Common Objenct in Context

セグメンテーション用データセット  
YOLOで使ってたような気がする

## ILSVRC

Imagenetを用いた画像分類コンペティション  
2010年スタートで色々やってて、2012年にAlexNetでCNNが入って来て色々とやってきている  
現在は終わって別の大会に引き継がれてる

## アーキテクチャとモデル

- アーキテクチャ : 学習前の構造だけのニューラルネットワーク  
- モデル : 上記に学習が入ったもの。学習後のネットワーク  

モデルという言い方は既に学習済みのNNを指すので注意

## 画像分類モデル

### AlexNet (2012)

ディープラーニング火付け役。スタンダートな構造だが、活性化関数にRelu関数が入ったもの。  
**初めてRelu関数やドロップアウトの技術が入ってきた、CNNのベースとなったネットワーク**

### VGG(2014)

よく使うやつ。
**max poolingで大域的な情報を圧縮するという考えが入ったモデル**  
また、フィルタサイズを減らし、ネットワーク層を増やすことで**受容野はそのまま、パラメータを減らして効率化が出来た**

### GoogleNet(2014)

**多層化のための工夫を織り込んだアーキテクチャ**  
Inception Moduleとか色々工夫が入ってる

#### Inception Module

大きなフィルタを小さいフィルタで分割計算することで、パラメータを減らした手法

#### Auxilirary Loss

勾配消失の問題を防ぐため、途中の中間層でもLoss計算、逆伝搬を行う手法  
これにより、疑似的なアンサンブル学習(疑似的な複数ネットワークでの学習)ができ精度向上となっている

#### GAP (Global Average Pooling)

各チャネルごとで平均を取る手法  
CAMでも出てきたやつ。これにより、最後のクラスへの出力部分で全結合層が無くせるのでパラメータ削減 + 過学習の防止になる

#### Pointwise Convolution

チャネル方向での畳み込み  
次元削減とかに使えたりする。確かMobileNetでも似たような物があったはず。

### Resnet (2015)

Skip Conectionなどが入ったResidual Blockを導入、残差を学習することで超多層化が出来るようになったネットワーク  
過去、自分が自論文で使ってたネットワーク

#### Residual Block

層を跨ぐ結合 Skip Connectionを用いて残差を学習するブロック  
入力がそのまま出力に入りこむため、層が深くなることに勾配計算 -> 微分により値が小さくなっていた問題が解決された  
**単純にskip入れるだけで勾配消失が改善するので、これ以降のモデルにもこの考えが入るようになった**

### Wide Resnet (2016)

Resnetのチャネル数を増やして計算量と層を減らして精度をupさせたResnet

### Dense Net (2016)

Skip Connectionを別レイヤー（後ろの層全て）に繋げられるようにし、
残差をさらに計算可能にしたモデル
Pooling的な処理はTransition Layerとして別枠に追加して対応している

### Mobile Net (2017)

計算量削減を実現したアーキテクチャ  
Depthwise Separable Convolutionと呼ばれる、畳み込み計算を更に分けた計算によって計算を削減

#### Depthwise Separable Convolution

平面方向の畳み込みとチャネル方向の畳み込みを分割して行うことで計算量削減を実現したConv計算

### Efficient Net (2019)

モデルの深さ/広さ/解像度を同時に調整するアーキテクチャ
MobileNetでハイパーパラメータとなっていた部分を自動的に探索、調整する形式になっている
#### MBConv

チャネルごとに重みづけを行い、MobileNetと同じConv計算を行う畳み込み計算

#### Compound Scaling Methods

深さ、広さ、解像度を定式化して、１つのハイパーパラメータとまとめて、
そのパラメータをグリッドサーチすることで、自動的に必要なパラメータを探索可能にする方式

## 物体検出モデル

### Selective Serch + SVN

**Deep Learning関係ないモデル**

何かがありそうな領域を大域的な範囲から局所的な範囲まで適当に取ってきてSVMに渡し、
SVN側で何かあるかどうか and それが何かを識別させていたモデル

#### ROIs(関心領域)

この辺に何かありそうだなを切り出した画像。  
方法としてSelective searchが用いられたりする
### R-CNN (2013)

Selective Searchで候補領域を探索したもの(ROIs)を入力とし、そのデータをCNNで解析して結果を出力するモデル。
ただし、最後の出力にSVMを使っていたりなどする部分もある。

### Fast R-CNN (2015)

ROIsではなく通常の画像を入力とし、そこから出た特徴マップに対してSelective Searchを用いてROIsを作成するモデル。
作成した後はSVMなり全結合層レイヤーなりで解析して分析を行う。

#### ROI Pooling

出力した特徴マップのROIsはそのままだと画像サイズ不一致で入力できないため、
それっぽい領域に4分割して、その平均値を用いて元の画像にする手法
### Faster R-CNN (2015)

Selective Searchを使ってROIsを使っていた部分もCNNを用いて抽出する形式にしたもの。
これにより、End to Endでの学習が可能となった

R-CNN系列は一環としてクソ重いという問題がある(1秒に5枚くらいしか画像分析できない)

### YOLO (2016)

You Only Look Once

過去、他の先輩が使っていたモデル。

「You Only Look Once(人生は一度きり)」というだけあって、
一度画像見ただけで画像の検出と識別を1回で行うようにしたモデル。

#### YOLOの解析方法

1. 7*7のバウンティボックスを画像全体に設定（豆腐みたいに画像全体をさいの目切りするようなイメージ）
2. バウンティボックス内にある物体を検出、ROIsをいくつか作成（さいの目ごとに検出処理）
3. 最後にバウンティボックスごとに重なったクラスを１つに絞らせる（重複した検出を消す）

さいの目切りを最初にする関係で、バウンティボックスの「位置」や「範囲」が制限される問題がある。
（密集した画像や様々な物体が重なった場合、とたんに弱くなる）

### SSD (2016)

Single Shot MultiBox Detectorの略

さいの目切りをいくつかのサイズで用意して切ることで、様々な候補領域を表現できるようにしたYOLOの改良版

### Mask R-CNN

Faster R-CNN改良版。

層を追加することで「物体検出」「インスタンスセグメンテーション」「姿勢推定」全てが同一のCNNで可能になったモデル

#### ROI Align

ROI Poolingで取れなかった部分を取れるように切り出し可能にしたやつ

#### Instance Segmantation

ただのマルチタスク学習に近い気がする。

物体検出にインスタンスセグメンテーション(個体ごとでの領域分割が入ったセグメンテーション)も入れ込んで精度向上させている

### FCOS

YOLO/SSD改良版

バウンティングボックスの設定を自動的にする（アンカーフリー）ことで、全体的な制度向上したモデル。

#### FPN (Featutre Pyramid Networks)

昔自作モデルで流用したモデル。

次元圧縮の後に特徴を増やすような層を追加、ボトムアップ的な情報も抽出しようとしたやつ

#### Focal Loss

数が少ないクラスに対しては重みをつけてLossを計算する手法

#### Center-ness

バウンティングボックスの中心に着目して、そこの値を予測+損失関数の重みにした手法
